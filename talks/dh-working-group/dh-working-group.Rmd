---
title: "The Migration of the Field Code"
author: "Kellen Funk and Lincoln Mullen"
output: tufte::tufte_handout
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, comment = NA)
library(tufte)
library(textreuse)
library(dplyr)
library(ggplot2)
library(ggthemes)
library(readr)
library(knitr)
library(igraph)
library(ggraph)
library(stringr)
source("../../R/helper.R")
source("../../R/section-matches.R")
source("../../R/spectrogram.R")
load("../../cache/corpus-lsh.rda")
load("../../cache/network-graphs.rda")
clusters <- read_csv("../../out/clusters.csv")
```

# The Field Code

After the American Revolution, most states were common law jurisdictions, sometimes with courts of chancery. These courts had a complex system of pleading defined mostly be case law. By the 1840s, lawyers and the mercantile classes called for the codification of civil procedure. At stake in the ensuing debates were the rationality of the law a science, or the purity of Anglo-Saxon civilization. New York was the first state to codify its procedure, thanks to the efforts of David Dudly Field. New York's 1848 Field code became by the end of the century the model for the procedural codes in state jurisdiction.

\begin{figure}
\includegraphics{../aha2016/field-code-states-map.jpeg}
\caption{Field Code states by date of enactment. Many states subsequently revised their codes of civil procedure.}
\end{figure}

Legal historians have long known that the Field code spread to other jurisdictions. Beyond the mere fact of its adoption, however, no one has studied the content of the borrowings. Which codes borrowed from each other? Which sections were borrowed, and which were modified? What were the patterns of borrowings and of innovations? 

To answer these questions we have gathered a corpus of 115 codes of civil procedure containing about 7.6 million words, and detected the borrowings algorithmically. Our approach is one of two common approaches. We have gathered a dataset of sources to answer a given set of questions; the other option is to take the sources as given and explore the data to see what questions it raises.^[Against the buzzword "big data," I call this "middle data", which we might define as data that is too small for distributed computation but too big for naive algorithms. Alternatively, it is data where the size of the sample approaches the size of the population, but where the population is strictly constrained by the research problem. Against big data, middle data offers the most promising way to combine traditional and digital history methods.]

# How we found the borrowings

We found out how the codes borrowed from one another by breaking the codes up into sections and comparing each section to every other section. We were justified in this approach because what we were doing was in essence repeating the process that nineteenth-century codifiers had taken. Newspapers and other observers mentioned the "the scissors and paste-pot " codes, and we have found in the archives codes which were marked up and edited by hand to be adopted for a different state. We think our method is successful because it mimics what we know to have happened by other historical methods.

## Preparing the corpus

Having identified all of the relevant codes of civil procedure in the nineteenth-century, including separately published codes, session laws, and statutes, we used OCR software to create plain-text versions of the codes.^[The data and code to re-run our analyses are available in a GitHub repository: <https://github.com/lmullen/civil-procedure-codes>.] These OCR files received only a light cleaning: we edited the section markers by hand as necessary, and wrote a script to fix the most obvious OCR errors.

We then split each section of each code into its own text file. The corpus contains nearly 98,000 sections. Below is a sample file containing a single section from a single code.^[Section 151 of the California's 1851 code of civil procedures from the file `CA1851-001660.txt`.]

```
151. An issue arises when a fact or conclusion of law 
is maintained by the one party, and is controverted 
by theothsr. Issues are of two kinds: lst. Of law: 
and,  _ 2d. or fact. if?
```

## Tokenizing and measuring similarity

The next stage in our process was to measure the similarity of the sections one to another. ^[This stage of our process was embedded in the [textreuse package](https://github.com/ropensci/textreuse) for R, which was peer-reviewed by rOpenSci: <https://github.com/ropensci/textreuse>.  See also my blog post, "[An Introduction to the textreuse Package, with Suggested Applications](http://lincolnmullen.com/blog/an-introduction-to-the-textreuse-package/)": <http://lincolnmullen.com/blog/an-introduction-to-the-textreuse-package/>. ] We first had to tokenize the text, meaning that  After experimenting we found that five-grams worked well.^[We also hashed the tokens, meaning that we converted them to integer representations which permits considerable savings of memory.]  The use of n-grams, which is a ubiquitious technique, permits redudancy of phrasing, intentional word changes, and OCR errors. Below are the first five tokens from the section above.

```{r}
tokenize_ngrams("151. An issue arises when a fact or conclusion of law 
is maintained by the one party, and is controverted 
by theothsr. Issues are of two kinds: lst. Of law: 
and,  _ 2d. or fact. if?", n = 5) %>% head(5)
```

Next we used the Jaccard similarity score for measuring document similarity. That measure, treating the tokens as a set, is the ratio of shared tokens to the ratio of total tokens in two documents. The result is a single number ranging from 0 (complete dissimilarity) to 1 (complete similarity).^[From MMDS. Formally, the definition of the Jaccard similarity score is $$J(A, B) = \frac{ | A \cap B| }{ | A \cup B|}$$] For instance, the section from the California 1851 code above was derived from section 756 of the New York 1850 code (`NY1850-008350.txt`). Because of changes to the wording and OCR errors, the Jaccard similarity between the two sections was `r jaccard_similarity(sections[["NY1850-008350"]], sections[["CA1851-001660"]]) %>% round(3)`. By carefully checking matching sections versus scores, we arrived at a rule of thumb that a Jaccard similarity score greater than 0.15 likely indicated a match, and a score greater than 0.2 almost certainly indicated a match.

## Computing similarity for the entire corpus

```{r}
num_comparisons <- function(n) { ((n * (n - 1)) / 2) }
billionize <- function(x) { round(x / 1e9, 1)}
```

The next step was to compute similarity for every section in the corpus. The difficulty is that to compare every section to every other section would require an enormous number of comparisons: approximately `r length(sections) %>% num_comparisons() %>% billionize()` billion for our corpus.^[Assuming that the similarity measure is bi-directional, the number of pairwise comparisons in a corpus is given by $(n^2-n) / {2}$.] Most of these comparisons would be wasted, since most sections have zero relationship to most other sections.

```{r, fig.margin = TRUE, fig.cap = "This chart shows the threshold S-curves for various settings of the minhash/LSH algorithm. The x-axis shows the actual measured Jaccard similarity of the two documents; the y-axis shows the probability that they will be marked as a match. We used the settings for the leftmost curve, guaranteeing that we detected all matches above a similarity of 0.2."}
prob_for_graph1 <- Vectorize(function(x) lsh_probability(120, 60, x))
prob_for_graph3 <- Vectorize(function(x) lsh_probability(120, 30, x))
prob_for_graph5 <- Vectorize(function(x) lsh_probability(120, 10, x))
input <- data_frame(x = seq(0, 1, 0.001))
ggplot(input, aes(x = x)) + 
  stat_function(fun = prob_for_graph1, linetype = 1) +
  stat_function(fun = prob_for_graph3, linetype = 2) +
  stat_function(fun = prob_for_graph5, linetype = 2) +
  theme_tufte() +
  labs(x = "Jaccard similarity",
       y = "Probability",
       title = "Probability of a match for a given similarity") +
  scale_x_continuous(breaks = seq(0, 1, .2))
```

We implemented the minhash/locality sensitive hashing (LSH) algorithm to detect candidate pairs, i.e., documents which were likely to be matches. This algorithm works by extracting a set number of random tokens from each document, allowing the documents to be represented uniformly and compactly. Then those random tokens are grouped into subsets. If any two documents have a matching subset, then they are considered a candidate pair. This algorithm has several useful properties. It approximates the Jaccard similarity of the two documents. The algorithm requires a computation for each document, not each pair of documents, so the compute time grows linearly not geometrically. And by controlling various parameters, one can determine a threshold similarity score above which one is likely to find a match and unlikely to find an inaccurate match.

Once we had detected the candidate pairs, we measured the actual Jaccard similarity for those pairs. The result was a sparse matrix of similarity scores, with rows and columns for each section in the corpus. A tiny subset is shown below.

```{r, fig.fullwidth = TRUE}
a_cluster <- clusters %>% filter(cluster_id == 676) %>% `$`("doc")
sample_matrix <- sections[a_cluster] %>% 
  pairwise_compare(jaccard_similarity, progress = FALSE, directional = TRUE) %>% 
  round(2) 
sample_matrix[is.na(sample_matrix)] <- ""
knitr::kable(sample_matrix, align = "c", 
             caption = "A subset of the similarity matrix")
```

This matrix, however, required further filtering based on what we knew about the process of borrowing from other methods. For instance, a code from 1851 could obviously not have borrowed from a code from 1877. Furthermore, in chains of borrowing (e.g., MT1895 $\rightarrow$ CA1868 $\rightarrow$ CA1851 $\rightarrow$ NY1850) the latest section might have a high similarity to all of its parents, but was in fact borrowed only from the most recent parent. We therefore filtered the similarity matrix to remove (1) matches within the same code; (2) anachronistic matches; (3) spurious matches beneath a certain threshold. Then if a section had multiple matches, we kept the match from (5) the chronologically closest code from the same state unless (6) there was a substantially higher match from a different code. The result was a sparse matrix of the most likely matches for each section.

# Learning from the borrowings

A similarity matrix is a common input to many other visualizations and algorithms. We used the matrix of best borrowings to learn how the codes were borrowed in several ways.

## Clustering the borrowings

We used a clustering algorithm to group similar sections together. There are innumerable clustering algorithms, but we needed one that could work with a sparse matrix and one whose assumptions aligned with the problem we were working on. We used the affinity propagation clustering algorithm.^[Brendan J. Frey and Delbert Dueck, "Clustering by Passing Messages Between Data Points," *Science* 315 (2007): 972--976.] That algorithm assumes that there is an "exemplar" item in each cluster. That assumption exactly matches the case with the Field code, where a single section (likely from NY1850) had many borrowings. Furthermore, even though the affinity propagation clustering algorithm did not fully converge on our peculiar dataset, it did do an adequate job of clustering the documents initially. Because there was an exemplar section for each cluster, we were able to merge clusters where the exemplar sections had a high Jaccard similarity score. 

The result was a set of approximately 2,900 clusters which contained at least five sections. This probably overstates the number of ur-sections in the corpus. Each cluster contained a list of the section IDs that belonged to it. The biggest cluster, for instance, contained 103 sections, which concerned the use of affidavits in pleading. Scholars in digital literary studies often speak of "deformance" of texts.^[Stephen Ramsay, *Reading Machines: Toward an Algorithmic Criticism* (University of Illinois Press, 2011), ch. 3.] Clustering the sections of the codes deformed them because it pulled the sections out of the codes from which they had been embedded in, and placed them in chronological order. This allowed us to see the development and spread of the law. You might consider this a method of distant reading which then permits a kind of close reading. Consider this (admittedly more important than usual) excerpt from one of the clusters which concerned the competence of witnesses:

```
Cluster ID: 10382          Documents in cluster: 20
Exemplar: OR1854-003380    Earliest: NY1850-018680

NY1850-018680 ------------------------------------------------------------------
1709. The following persons are not admissible: 1. Those who are of unsound mind
at the time of their production for examination: 2. Children under ten years of
age, who appear incapable of receiving just impressions of the facts, respecting
which they are examined, or of relating them truly.

CA1851-004380 ------------------------------------------------------------------
394. The following persons shall not be witnesses: â€˜ lst. Those who areof
unsound mind at the time of their production for examination: 2d. Children under
ten years of age, who appear incapable of receiving just impressions of the
facts respecting which they are examined, or of relating them truly: and, 3d.
Indians, or persons having one fourth or more of Indian blood, in an action or
proceeding to which a white person is a party: 4th. Negroes, or persons having
one half or more Negro blood, in an action or proceeding to which a white person
is a party.

OR1854-003380 ------------------------------------------------------------------
6. The following persons shall not be competent to testify 1. Those who are of
unsound mind, or intoxicated at the time of their production for examination ;
2. Children under ten years of age, who appear incapable of receiving just
impressions of the facts respecting which they are examined, or of relating them
truly; 4. Negroes, mulattocs and Indians, or persons one half or more of Indian
blood, in an action or proceeding to which a white person is a party.
```

## Networks of borrowings

A matrix of similarities can be thought of as an adjacency matrix to a network graph. We created network graph of the code to code borrowings. In an effort not to make this a typical hairball, we filtered the edges so that each code was connected to another code only if it borrowed at least fifty sections.

```{r, fig.fullwidth=TRUE, fig.cap="Code to code borrowings.", fig.height=3}
set.seed(21000)
ggraph(codes_g, "igraph", algorithm = "nicely") +
  geom_edge_fan(aes(edge_width = sections_borrowed, 
                    alpha = sections_borrowed),
                arrow = arrow(type = "closed", ends = "first",
                              length = unit(0.10, "inches"),
                              angle = 15)) +
  geom_node_point(aes(color = as.factor(distance)), size = 2) +
  scale_edge_width("Sections borrowed", range = c(0.25, 1), guide = "none") + 
  scale_edge_alpha(range = c(0.3, 0.6), guide = "none") +
  scale_color_brewer(palette = "Set1", "Distance from a NY code") +
  ggforce::theme_no_axes(base_size = 4, base_family = "serif") +
  geom_node_text(aes(label = name), size = 1.5) +
  theme(legend.position = "bottom", 
        panel.border = element_blank())
```

This network shows the structure of the borrowings. In particular, it demonstrates the centrality of New York's code, especially the eponymous Field Code of 1850. The New York code then became the basis of other families of codes. Those codes heavily edited NY1850, but within the families (which were often based on the proximity of states) the borrowings tended to be much more close. Yet states could and did sometimes borrow from multiple states. Notice that this network graph moves both geographically an chronologically. It is later codes which tend to be on the outside of the network diagram, and they are the most distant from the original Field Code in both time and borrowings.^[If we move up a level of abstraction we can consider not code to code but state to state borrowings. This shows us the pattern of borrowings more clearly, albeit at a considerable loss of detail. While that is a constant challenge in any kind of visualization work, it is especially a challenge here where we know that our measures of borrowings can be inaccurate for individual sections, though they are reliable on the whole.]

```{r, eval=FALSE, fig.fullwidth=TRUE, fig.cap="State to state borrowings.", fig.height=3}
set.seed(210)
ggraph(states_g, "igraph", algorithm = "graphopt") +
  geom_edge_fan(aes(edge_width = sections_borrowed, 
                    alpha = sections_borrowed),
                arrow = arrow(type = "closed", ends = "first",
                              length = unit(0.10, "inches"),
                              angle = 15)) +
  geom_node_point(size = 3, aes(color = region)) +
  scale_edge_width("Sections borrowed", range = c(0.25, 1), guide = "none") + 
  scale_edge_alpha(range = c(0.3, 0.6), guide = "none") +
  scale_color_brewer(palette = "Dark2", "Region") +
  ggforce::theme_no_axes(base_size = 4, base_family = "serif") +
  geom_node_text(aes(label = name), size = 1.5) +
  theme(legend.position = "bottom", 
        panel.border = element_blank())
```

## Borrowings within each code

Finally, each of those nodes on the network diagram can be individualy investigated to get a more detailed perspective on how the codes were borrowed. We did this by creating a kind of "spectrograph" or fingerprint of which sections in a code were borrowed from which other codes. Take the case of Washington state's 1855 code. 


```{r, fig.cap="Borrowed sections in Washington's 1855 code. Each section in the code is represented by a square, the color of which indicates where it was borrowed from."}
spectrogram("WA1855", best_matches, white_list = 4)
```

We can observe that most of the code is borrowed from some source or another. While there is a fair bit of "other" which are probably sections which we cannot adequately attribute, the pattern of borrowing is clear: IN1852 and OR1854 provide the majority of the borrowings, and they do so in two bands. This pattern is curious, because one might expect Washington to borrow from Oregon and California but Indiana is not an obvious choice. And why do the borrowings fall into these bands? Regulations on judgment were borrowed from Oregon, while enforcement provisions came from Indiana. We think this is because one of the Washington code commissioners, Edward Lander, was an Indiana appelate judge from 1850 to 1853, while another commissioner, William Strong, was a justice of the Oregon Supreme Court in the same years. In other words, the law in this case moved with judges and lawyers who picked the parts they knew best.

Of course we can also demonstrate in an obvious way when codes were borrwed heavily from one source:

```{r}
spectrogram("NV1861", best_matches, white_list = 4)
```

# Takeaways

Methodological:

- harmony of methods
- applicability to future

